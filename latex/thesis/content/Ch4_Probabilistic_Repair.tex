\chapter{\rm\bfseries Probabilistic Program Repair}
\label{ch:chapter03}

As we have seen, the problem of program repair is highly underdetermined. To resolve this ambiguity, we will use a probabilistic model to induce a distribution over the language of valid programs. This distribution will guide the repair process by assigning a likelihood to each possible repair. Then, taking the maximum over all possible repairs, we can find the most likely repair consistent with the constraints and the observed program.

Specifically, we will define an ordering over strings by their likelihood under the probabilistic model. We then define a repair as the most likely string consistent with the observed program and the grammar. We factorize the probability of a string as the product of the probability of each token in the string, conditioned on its predecessors. This allows us to compute the joint probability in a left-to-right fashion.

This probabilistic model will generally admit programs that are locally probable, but globally inconsistent with the grammar. To enforce syntactic validity, we will use the probabilistic language model to ``steer'' a generative sampler through the automaton representing the repair language. This has two advantages: first, it allows us to sample from the repair language incrementally, and second, it ensures that subsequences with high probability are retrieved first, and all trajectories are syntactically valid.

We will consider two kinds of probabilistic models: a constrained Markov model and an unconstrained transformer-based neural network trained on program repair, then evaluate the performance of these models on a syntax repair benchmark consisting of pairwise program transformations. As we will show, the constrained Markov model is able to achieve state-of-the-art precision on blind prediction of the lexical sequence.

\begin{figure}[H]
  \resizebox{.24\textwidth}{!}{\input{../popl2025/len_dist_tidy}}
  \resizebox{.24\textwidth}{!}{\input{../popl2025/len_dist_bifi_all}}
  \resizebox{.24\textwidth}{!}{\input{../popl2025/len_dist_s2p}}
  \resizebox{.24\textwidth}{!}{\input{../popl2025/len_dist_bifi}}
  \caption{Tidyparse, Seq2Parse and BIFI repair precision across length and edits.}
\end{figure}

If we give it an equivalent number of samples, the constrained Markov model attains an even wider margin.

\begin{figure}[H]
  \resizebox{.24\textwidth}{!}{\input{../popl2025/len_dist_tidy}}
  \resizebox{.24\textwidth}{!}{\input{../popl2025/len_dist_bifi_all}}
  \resizebox{.24\textwidth}{!}{\input{../popl2025/len_dist_tidy200}}
  \resizebox{.24\textwidth}{!}{\input{../popl2025/len_dist_tidy20k}}
  \caption{Tidyparse, Seq2Parse and BIFI repair precision across length and edits.}
\end{figure}

Now, we measure latency.

\begin{figure}[H]
%    \resizebox{.19\textwidth}{!}{\input{bar_hillel_repair.tex}}
  \resizebox{.24\textwidth}{!}{\input{../popl2025/bar_hillel_repair_1}}
  \resizebox{.24\textwidth}{!}{\input{../popl2025/bar_hillel_repair_2}}
  \resizebox{.24\textwidth}{!}{\input{../popl2025/bar_hillel_repair_3}}
%    \resizebox{.24\textwidth}{!}{\input{bar_hillel_repair_5}}
%\resizebox{.3\textwidth}{!}{\input{repair1_plot.tex}}
%\resizebox{.307\textwidth}{!}{\input{repair2_plot.tex}}
  \caption{Latency benchmarks. Note the varying axis ranges. The red line marks Seq2Parse and the orange line marks BIFI's Precision@1 on the same repairs.}\label{fig:human}
\end{figure}