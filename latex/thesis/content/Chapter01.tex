\chapter{\rm\bfseries Formal Language Theory}
\label{ch:chapter01}

In computer science, it is common practice to conflate two distinct notions for a set. The first is a collection of distinct objects sitting on some storage device. The second is a lazy construction: not as an explicit collection of objects, but a representation, that, when coupled with some computational resource, allows us to efficiently determine the members of the set as needed. This second allows us to represent infinite sets, without requiring an infinite amount of memory to store them. Inclusion then, instead of being simply a lookup query as in the first case, becomes a decision procedure. This is the basis of formal language theory, which is the study of sets of sequences defined by a representation.

The representation we are chiefly interested in are grammars, which are a common metanotation for specifying the syntactic constraints on programs, shared by nearly every programming language. Grammars are generally overapproximations to the language, providing a fast procedure for rejecting invalid programs, but requiring additional semantic analysis to decide inclusion.

Like sets, it is possible to combine grammars using set operations. This allows us to perform operations like union, intersection, and difference on languages. These operations are useful for combining syntactic and semantic properties of programs. For example, we might have two grammars, representing two properties that are both necessary for a program to be correct. Then we can encode all the programs that satisfy both properties by intersecting the two languages.