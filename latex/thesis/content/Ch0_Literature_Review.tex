\chapter{\rm\bfseries Related Literature}
\label{ch:litreview}

Translating ideas into computer programs demands a high degree of precision, as computers have strict criteria for admitting valid programs. These constraints act as a failsafe against faulty programs and runtime errors, but can be tedious to debug. During the editing process, these constraints are invariably violated by the hasty or inexperienced programmer, requiring manual repair. To assist with this task, automated program repair (APR) attempts to generate possible revisions from which the author may choose. This subject has been closely investigated by programming language research and treated in a number of existing literature reviews~\cite{monperrus2018living, le2021automatic}. We direct our attention primarily towards syntax repair, which attempts to fix parsing errors, the earliest stage in program analysis.

\section{Syntax Repair}

Spellchecking is an early precursor to syntax repair was originally developed for word processing and seeks to find, among a finite dictionary, the most likely intended revision of a misspelled word~\cite{kernighan1990spelling}. Similarly, syntax repair considers the case where this dictionary is not necessarily finite, but rather generated by a grammar representing a potentially infinite collection of words called a \textit{language}. This has applications in natural language processing~\cite{bryant2023grammatical}, although we are primarily interested in programming languages. In the case of programming language syntax, the language and corresponding grammar is typically context-free~\cite{chomsky1959algebraic}.

Various methods have been proposed to handle syntactic program errors, which have been a longstanding open problem since the advent of context-free languages. In 1972, Aho and Peterson~\cite{aho1972minimum} first introduce an algorithm that returns a syntactically valid sequence whose distance from the original sequence is minimal. Their method guarantees that a valid repair will be found, but only generates a single repair and does attempt to optimize the naturalness of the generated solution.



While algorithmically elegant, deterministic methods are too rigid to model the natural structure of source code. A pragmatic solution must not merely suggest parseable repairs, but also generate suggestions a human is likely to prefer in practice. To model coding conventions, stylistic patterns and other programming idioms that are not captured in the formal grammar, researchers have borrowed techniques from natural language processing.

Recent work Yasunaga et al.~\cite{yasunaga2021break} and Sakkas et al.~\cite{sakkas2022seq2parse} use natural language models to generate probable fixes.

These models are capable of learning statistical patterns, but often sacrifice validity, precision or latency as these techniques are prone to misgeneralize and hallucinate syntactically invalid repairs.
These models do not attempt to sample from the space of all and only valid repairs. As a consequence, they have difficulty with inference scaling, where additional test time compute does not translate into improved precision. Furthermore, the generated samples may not even be syntactically valid, as we observe in practice.


Our work addresses all of these concerns. We try to generate every nearby valid program and prioritize the solutions by naturalness, while ensuring response time is tolerable. In other words, we attempt to satisfy soundness, completeness, naturalness and latency simultaneously.


\clearpage